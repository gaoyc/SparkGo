spark总结
==========

# 术语
##　Ｓpark术语  
- DAG(Directed Acyclic Graph)/有向无环图
  > 在图论中，如果一个有向图无法重任意顶点触发经过若干条边回到该顶点，则这个图就是一个有向无环图(DAG图), Spark中每一个顶点代表一个任务，没一条边代表一种限制u约速条件（Spark中的依赖关系）
  - DAG的生成机制
    > 生成过程的重点是Stage的划分，划分依据是RDD的依赖关系.窄依赖被划分为同一个Stage。对于宽依赖，由于shuffle的存在，必须等待父RDD的shuffle处理完成才能开始接下来的计算，所以会在此处进行Stage划分。
    - DAG生成相关源代码: DAGSCheduler.scala的getParentStages方法
- 算子
  > 是RDD中定义的函数,可以对RDD中的数据进行转换或者操作
  - Transformation算子
    > 是一种算法描述.标记着需要进行操作的数据,但不真正执行。具有Lazy特性，草足都是延迟计算的，需要等待有action操作或者Checkpoint操作是，才真正触发操作
  - Action算子
    > 是一种算法描述.它通过SparkContext触发一个Job向用户程序返回一个值或者结果。
    action算子会触发SparkContext的runJob方法提交作业,触发RDD DAG的执行并将数据输出到Spark系统。
- SparkContext: 
  - 实例化SchedulerBackend,底层调度器TaskScheduler, DAGScheduler
  - 一个JVM只能启动一个启动的sparkContext，要启动新的SparkCntext必须先关闭原有的
- Job/作业: 有action触发
- Stage: 
  - 一个Stage的开始就是从外部储存或者shuffle结果中读取数据，一个Stage的结束是由于要发送shuffle或者生成最终的计算结果
  - 一个Job需要查分成多组任务来完成，每组任务有stage封装.
  - 每当shuffle会产生新的stage，每一个stage封装一个taskSet，然后以一个个taskSet的形式提交到底层调度器(TaskScheduler)
  - 是Spark Job运行是具有相同逻辑功能且并行计算任务的一个基本单元。Stage中所有任务都依赖同样的shuffle，每个DAG任务通过DAGSchduler在Stage的边界处发生shuffle形成Stage，然后DAGScheduler运行这些阶段的拓扑顺序。
  - 类型有ShuffleMapStage和ResultStage
    - ShuffleMapStage
        > 是DAG产生数据进行Shuffle的中间阶段，它发生在每次Shuffle操作之前，可能包括多个pipelined操作
    - ResultStage
        > RsultStage阶段捕获函数在RDD的分区上进行的Action算子计算结果，有些Stages并不是运行在RDD的所有分区上，如first(), lookup()等
  
- TaskSet/任务集: 一组任务就是一个TaskSet, 对应一个Stage.一个taskSet的所有Task之间没有shuffle依赖，因此相互之间可以并行运行
- Task/任务
  > 一个独立的工作单元.有Driver发送到Executor上执行。根据Task返回类型的不同，Task又分为shuffleMapTask和ResultTask
  - Task运行在Executor之上，而Executor为于CoarseGrainedExecutorBackend(JVM进程)中
  - Task的分类: 根据Task所处的Stage的位置分两大类
    - shuffleMapTask
      > 指Task所处的Stage不是最后一个Stage, 也就是Stage的计算结果还没有输出，而是通过shuffle交给下一个Stage使用
    - resultTask
      > Task所处的Stage是DAG中的最后一个Stage，计算结果需要输出，计算到此为止。除最后一个Stage的Task为resultTask外，其他的所有task都是shuffleMapTask
- SparkListener
  > 是spark调度器的事件监听接口,注意该该接口会随着Spark的版本的不同而变化
- DAGScheduler: 由SparkContext启动,是面向Stage的高层级的调度器。构建DAG，拆解Tasks，构建stage，将一个个TaskSet（每个stage封装成一个taskSet）提交到底层调度器(TaskScheduler)。
  需要记录哪些RDD被存入磁盘等物化动作，同时需求Task的最优调度(例如数据就近计算，计算向数据移动)，还需要监视因为shuffle跨节点输出可能导致的失败，stage任务重新计算（与task一样，默认重试4次）
- BlockManager
- Wide Dependency/宽依赖(shuffle dependency)
  - 多个chlid partition会依赖于同一个parent RDD的partition
  - 宽依赖意味着shuffle操作,会导致计算是产生shuffle操作的RDD操作，是spark划分stage的依据。典型的操作有goupByKey,sortByKey等
  - 宽依赖支持两种shuffle Manager: HashShuffleManger与SortShuffleManager。前者基于Hash的shuffle机制，后者基于排序的shuffle机制
- Narrow Dependency/窄依赖
  - 每一个parent RDD的partition最多被一个child RDD的一个partition所使用
  - 有两类:
    - OneToOneDependency/一对一依赖. 典型的map，filter等操作
    - RangeDependency/范围依赖. 典型的union
    - 两者最大区别是出现了outStart，length， instart参数

- StorageLevel/储存级别
  > 要点说明,例如MEMORY_AND_DISK_SER_2, SER表示序列化，最后的数字表示副本数，1个副本不带数字后缀。
  - 只能被设置一次，从None修改为一个值后，就不能在修改
  - MEMORY_ONLY: 表示数据只储存在内存中，如果不能被内存装下，不能装下的部分分区不被缓存，并在需要时重新计算。Spark默认模式
    
- Executor
  - 计算节点执行计算逻辑时，复用位于Executor中的线程池中的线程，线程中运行的任务是调用具体Task中的run方法进行计算，此时如果调用具体Task的run方法，需要考虑不同Stage内部具体Task的类型，Spark规定最后一个Stage中的task类型为resultTask，因为需要或者最后的结果，前面所有stage的Task都是shuffleMapTask

## Spark SQL相关术语  
- Parquet文件
  一种支持多种数据处理系统的存储格式.
- DataFrame
  - 是以指定列(named columns)组织的分布式数据集合,在Spark Sql中，相当于关系型数据库的一个表(二维表格)
  - 是以RDD为基础的分布式数据集,带有schema元信息，及DataFrame所表示的二维数据集的每一列都带有名称和类型。
- SerDes: 用户定义的序列号格式.Hive的serialization和deserialization
- UDF: 用户自定义函数
- UDAF: 用户自定义聚合函数
- Explain: 解释
- Sampling: 抽样
- tungsten
  ？u由spark自己管理内存，而不是JVM，内存中对象被存储成Spark自己的二进制格式，计算直接发生在二进制上。更紧凑